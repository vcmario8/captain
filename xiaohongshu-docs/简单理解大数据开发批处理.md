# 简单理解大数据开发批处理

## 单机批处理——Unix 哲学

cat file.txt | awk -F ' ' '{print $3}' | sort | uniq -c
* cat file.txt——读入数据
* awk -F ' ' '{print $3}' ——mapper 操作
* 排序
*  uniq -c ——reducer 操作

整个过程是流处理，每个输出直接送到下个步骤的输入；直接在内存buffer 中操作，而不用写磁盘。

## 多机的批处理——MapReduce

MapReduce 的作业流程：
1. 读取一组文件，将其分解成一行行 records。
2. 调用 mapper 函数，遍历处理每一个 record。对应unix 例子中awk
3. 按 key 排序，对应unix 例子中 sort
4. 调用 reducer 函数，遍历处理每一个 record，如果出现多个 record 对应同一个 key，这些 record 会相邻。对应 unix 例子中的 uniq

## 多个批处理任务——workflow

一个批处理作业只能处理一个 key 的需求，比如你是站长，需要统计网站一天访问者的性别、城市、年龄等分布，一个 MapReduce 任务只能处理一个，你需要配置多个任务，然后串起来流式执行；将 MapReduce作业串联起来称为 workflow，一个作业的输出是另一个作业的输入，Hadoop 中是通过目录名来实现的，第一个作业必须配置输出为特定的 hdfs 目录，第二个作业的输入配置为同一个目录。

这个和 unix 管道的流处理不同，unix 管道是输出直接送给输入，而 hadoop 是写入了临时文件。